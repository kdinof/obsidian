---
title_en: "AI VS. HUMAN HEURISTIC EVALUATION: A PRACTICAL PLAYBOOK"
title_ru: AI VS Человек в дизайн ревью
tags:
language is RU: true
type:
status:
  - published
date-published:
---


ЧЕЛОВЕК ПРОТИВ AI: КТО ЛУЧШЕ КРИТИКУЕТ ДИЗАЙН?

Недавно искал что-то свежее на тему точности AI в анализе интерфейсов Наткнулся на исследование ребят из университета Вашингтона, где разбирали, кто лучше находит косяки в мобильных интерфейсах - живой эксперт или AI.

За основу критики брали 10 эвристик Нильсена. На ревью было два мобильных приложения. Оценивали пять человек-экспертов против GPT-4 и других моделей вроде Claude и Gemini. 

В итоге GPT-4 нашёл 73% и 77% всех багов и UX-проблем на двух приложениях. Для сравнения, у эксперты - 57% и 63%. GPT-4 не просто работает на уровне "тут цвет странный, тут кнопка маленькая". Он стабильно видит больше ошибок, без потери качества. В сравнении эксперты под конец длинного теста теряли фокус.

Естественно, не всё так идеально и AI бывает тупит: может принять системный индикатор iOS за часть приложения или не заметить специфический паттерн. А ещё находит кучу "проблем", которых на самом деле нет. Ещё GPT-4 пока плохо справляется с анализом не одного экрана, а флоу.

Короче говоря, я думаю и верю в то, что точность нахождения базовых косяков в дизайне с помощью LMM будет расти. Это поможет джунам расти быстрее fail-to-learn, вайбкодерам и не дизайнерами делать продукты качественнее.

Идеи из этого исследования я уже взял в работу над своим ботом для ревью интерфейсов. Можете потестить

КОРОТКО: AI не забирает работу — он делает ревью доступным и помогает расти быстрее. А лучшие продукты всё равно появляются на стыке технологий и человеческой экспертизы.

А вы бы доверили AI ревью вашего дизайна?

===

Человеческая vs AI оценка интерфейсов

Недавно искал статьи и исследования и точности AI в распознавании картинок и применении этого скилла в дизайне. Наткнулся на исследование ребят из университета Вашингтона о том, на сколько лучше или хуже AI умеет критиковать интерфейс, чем человек. 

За основу критики они использовали 10 эвристик Нильсена.

Соревновалась AI и 5-тью дизайн экспертами. Оценивали 2 мобильных приложения.

По итогам моделька GPT-4 нашёла **73% и 77% проблем**, что выше, чем средние показатели 5 экспертов (57% и 63%). Более того, AI показал стабильные результаты на протяжении нескольких месяцев, тогда как эксперты уставали и теряли внимание к деталям с увеличением объема работы. Также сравнивались другие модели (Claude, Gemini), однако GPT-4 продемонстрировал лучшие результаты по всем основным метрикам.

Вместе с тем, у AI есть свои ограничения. Модель иногда не распознаёт отдельные элементы интерфейса (например, системные индикаторы iOS ошибочно считает частью дизайна приложения) или не улавливает стандартные паттерны и конвенции отрасли. Существенная проблема — генерация большого числа «ложных» проблем (severity 0): AI может фиксировать то, что эксперты не считают реальными нарушениями, что увеличивает объем ручной фильтрации результатов. Кроме того, AI пока хуже справляется с анализом сквозных сценариев и оценкой последовательности между несколькими экранами.

Короче говоря, я думаю и верю в то, что точность нахождения базовых косяков в дизайне с помощью LMM будет расти. Это поможет джунам расти быстрее fail-to-learn, вайбкодерам не дизайнерами делать продукты качественнее. 

Идеи из этого исследования забрал в свой бот, который, как раз, проводить ревью интерфейса. Апдейты по нему тоже напишу в следующем посте. 






